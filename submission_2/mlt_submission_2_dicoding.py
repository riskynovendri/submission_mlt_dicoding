# -*- coding: utf-8 -*-
"""MLT Submission 2 Dicoding

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CKQFCJ-Xuq4Vt4XGs7ALOQivlwH5oxpm

System Rekomendasi Buku menggunakan Collaborative Filtering - Risky Novendri

# Load Dataset

Download dataset dari kaggle dan pindahkan ke dalam Google drive
"""

#Set kaggle API
import os
os.environ['KAGGLE_USERNAME'] = "riskynovendri"
os.environ['KAGGLE_KEY'] = "dd57766230ed28e08a6bbed6903c7745"

#Download dataset
!kaggle datasets download -d arashnic/book-recommendation-dataset

#Move to Gdrive
!mv /content/book-recommendation-dataset.zip /content/drive/MyDrive/

import zipfile
zip_path = '/content/drive/MyDrive/book-recommendation-dataset.zip'

## Extract Dataset
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall('/content/drive/MyDrive/MLT Submission 2 Dataset')

"""Import library yang dibutuhkan untuk membangun sistem rekomendasi"""

#Import Library
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from scipy.sparse.linalg import svds
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, concatenate, Dense, Flatten, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras import metrics
from tensorflow import keras

"""Load semua dataset yang ada yaitu User, Rating dan Books"""

users = pd.read_csv('/content/drive/MyDrive/MLT Submission 2 Dataset/Users.csv')

users

rating = pd.read_csv('/content/drive/MyDrive/MLT Submission 2 Dataset/Ratings.csv')

book = pd.read_csv('/content/drive/MyDrive/MLT Submission 2 Dataset/Books.csv')

rating

book

"""# Data Understanding

Mengecek Kondisi dataset yang dimiliki
"""

# Cek jumlah unique dataset
print("Jumlah Unique {} dari {} pada data users".format(len(users['User-ID'].unique()),len(users['User-ID'])))
print("Jumlah Unique {} dari {} pada data rating".format(len(rating['User-ID'].unique()),len(rating['User-ID'])))
print("Jumlah Unique {} dari {} pada data rating".format(len(rating['ISBN'].unique()),len(rating['ISBN'])))
print("Jumlah Unique {} dari {} pada data book".format(len(book['ISBN'].unique()),len(book['ISBN'])))
print(max(rating['User-ID']))
print(min(rating['User-ID']))
print(max(rating['Book-Rating']))
print(min(rating['Book-Rating']))

"""Mengecek informasi tipe data dan kolom yang ada"""

users.info()

rating.info()

book.info()

rating.rename(columns={"User-ID" : "user_id", "Book-Rating" : "book_rating"}, inplace=True)

rating['book_rating'].hist()

"""Persebaran data pada book rating terlalu banyak yang 0 dengan asumsi user tidak atau belum memberikan rating pada buku tersebut. Untuk data rating 0 dianggap tidak memiliki makna apapun maka buku dengan rating 0 akan di hapus

EDA data rating
"""

rating.describe()

rating_clean = rating[rating['book_rating'] > 0].sort_values('user_id', ascending=True)

rating_clean['book_rating'].hist()

"""Distribusi pada data terlihat condong ke kiri dimana banyak user yang memberikan rating tinggi"""

rating_clean.describe()

"""Hasil statistik terlihat lebih baik setelah di cleansing"""

df = rating_clean.sample(n=20000, random_state=42)

"""Batasi dataset karena masalah keterbatasan resources google colab yang tidak mampu mengolah lebih banyak data

# Data Preparation

Transformasi data yang diperlukan untuk Collaborative fitering dan Conten Based Filtering
"""

#collaborative filtering
user_item_matrix = df.pivot_table(index='user_id', columns='ISBN', values='book_rating').fillna(0)

"""Encode atribut ISBN dan User ID"""

# Mengubah ISBN menjadi list tanpa nilai yang sama
books_ids = df['ISBN'].unique().tolist()

# Melakukan proses encoding ISBN
books_to_books_encoded = {x: i for i, x in enumerate(books_ids)}

# Melakukan proses encoding angka ke ISBN
books_encoded_to_books = {i: x for i, x in enumerate(books_ids)}

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = df['user_id'].unique().tolist()
print('list userID: ', user_ids)

# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}

# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

# Mapping user ke dataframe df
df['user'] = df['user_id'].map(user_to_user_encoded)

# Mapping ISBN ke dataframe df
df['isbn'] = df['ISBN'].map(books_to_books_encoded)

"""# Content Based Filtering

Load Dataset
"""

# Membuat Dataframe Book
df_books = book[['ISBN','Book-Title','Book-Author']]
df_books.rename(columns={"Book-Title" : "book_title", "Book-Author" : "book_author"}, inplace=True)

df_books

from sklearn.feature_extraction.text import TfidfVectorizer

"""Data Preparation

Pada data terdapat beberapa nilai null sehingga perlu di bersihkan
"""

df_books.isnull().sum() #Cek data null

df_books.dropna(inplace=True) #Drop Null

df_books.isnull().sum()#Cek kembali nilai null

"""Melakukan drop duplicates untuk menghindari duplikasi data"""

df_books.drop_duplicates(subset='book_title',inplace =True) #Drop judul buku yang duplicate

#Cek Unique author
print(len(df_books))
print(len(df_books['book_author'].unique()))

df_books[df_books['book_author'].str.contains('Koontz', case=False)]['book_author'].unique() #Cek author nama koontz

"""Author bernama koontz memiliki nama yang tidak konsisten sehigga di hitung unique berbeda"""

#Membuat author jadi konsisten
df_books = df_books.replace('Dean Koontz', 'Dean R. Koontz')
df_books = df_books.replace('DEAN KOONTZ', 'Dean R. Koontz')
df_books = df_books.replace('Dean R. (Dean Rae) Koontz', 'Dean R. Koontz')
df_books = df_books.replace('Dean R Koontz', 'Dean R. Koontz')

df_books[df_books['book_author'].str.contains('Koontz', case=False)]['book_author'].unique() # Cek kembali

df_books = df_books.sample(n=20000, random_state=42) # Ambil sample random dikarenakan keterbatasan resources

"""Melakukan perhitungan TF-IDF untuk dapat menghitung cosine similiarity"""

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()

# Melakukan perhitungan idf pada data books
tf.fit(df_books['book_author'])

# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names_out()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
book_features_tfidf = tf.fit_transform(df_books['book_author'])

# Melihat ukuran matrix tfidf
book_features_tfidf.shape

from sklearn.metrics.pairwise import cosine_similarity

# Kalkulasi pairwise cosine similarity dengan book_author
cosine_similarities = cosine_similarity(book_features_tfidf)

# Membuat dataframe dari variabel cosine_similarities dengan baris dan kolom berupa nama buku
cosine_sim_df = pd.DataFrame(cosine_similarities, index=df_books['book_title'], columns=df_books['book_title'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap buku
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

# Merge the dataframes based on the "ISBN" column
merged_data = pd.merge(rating_clean, df_books, on="ISBN", how="inner")

# Display the merged dataframe
merged_data

data = merged_data[['user_id','book_rating','book_author','book_title']]

data

user_profile = data[data['user_id']== 39] # Ambil sample user profile dengan id 39

# user_profile = user_profile[['book_title']] #ambil judul buku

# book_indices

"""Membuat fungsi untuk dapat memberikan rekomendasi dari hasil perhitungan Cosine Similiarity"""

def book_recommendations(judul_buku, similarity_data=cosine_sim_df, items=df_books[['book_title','book_author']], k=5):
    # Mengambil data dengan menggunakan argpartition
    index = similarity_data.loc[:,judul_buku].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop judul_buku agar judul buku yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(judul_buku, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""Melakukan test rekomendasi pada model"""

user_profile['book_title'].head(1).item() # cek judul buku yang dimiliki user

user_profile.head(1)

book_recommendations(user_profile['book_title'].head(1).item()) #test rekomendasi

"""# Modelling Using Collaborative Filtering

Memulai untuk menghitung dan menerapkan SVD
"""

user_item_matrix

user_item_matrix = user_item_matrix.values  # Convert to NumPy array

# Apply Singular Value Decomposition (SVD)
U, sigma, Vt = svds(user_item_matrix, k=20)

# Convert sigma kedalam bentuk diagonal matrix
sigma_diag_matrix = np.diag(sigma)

# Rekonstruksi ratings matrix
predicted_ratings = np.dot(np.dot(U, sigma_diag_matrix), Vt)

"""Membuat Layer Model sesuai dengan kebutuhan"""

# Step 3: Deep Learning Model
embedding_dim = 100  # Adjust as desired

# Define input layer
input_layer = Input(shape=(2,), name='input')

# Embedding layers
embedding = Embedding(input_dim=user_item_matrix.shape[0] + user_item_matrix.shape[1], output_dim=embedding_dim,embeddings_initializer = 'he_normal',embeddings_regularizer = keras.regularizers.l2(1e-6))(input_layer)
flatten = Flatten()(embedding)

# # Concatenate embeddings
# concat = concatenate([user_embedding, book_embedding])
# flatten = Flatten()(concat)

# Dense layers
# Dense layers
dense1 = Dense(64, activation='relu')(flatten)
dense2 = Dense(32, activation='relu')(dense1)
dropout1 = Dropout(0.3)(dense2)
dense3 = Dense(256, activation='relu')(dropout1)
dropout2 = Dropout(0.4)(dense3)
dense4 = Dense(128, activation='relu')(dropout2)
dense5 = Dense(32, activation='relu')(dense4)
output = Dense(1, activation='linear')(dense5)

# Define the model
model = Model(inputs=input_layer, outputs=output)
model.compile(loss = tf.keras.losses.Huber(),
    optimizer = Adam(learning_rate=0.00001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()])

model.summary()

"""Membagi dataset training dan testing"""

X = df[['user','isbn']].values
y = df['book_rating'].values

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

from tensorflow.keras.callbacks import ModelCheckpoint
# Define a callback to save the model with the lowest validation loss (MSE)
checkpoint_callback = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)

History = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=32, callbacks=[checkpoint_callback])

"""# Evaluation"""

import matplotlib.pyplot as plt
plt.plot(History.history['root_mean_squared_error'])
plt.plot(History.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Selama proses learning performa model semakin membaik dan juga RMSE yang dihasilkan juga sangat kecil"""

# Load the best model
best_model = tf.keras.models.load_model('best_model.h5')

# Evaluate the best model
loss, rmse = best_model.evaluate(X_val, y_val)
print('Loss:', loss)
print('RMSE:', rmse)

"""Melakukan testing sistem rekomendasi yang sudah di terapkan"""

book_df = book[['ISBN','Book-Title','Book-Author']]
book_df.rename(columns={"Book-Title" : "book_title", "Book-Author" : "book_author"}, inplace=True)
df = rating_clean.sample(n=20000, random_state=42)

# Mengambil sample user
user_id = df.user_id.sample(1).iloc[0]
book_read_by_user = df[df.user_id == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
book_not_read = book_df[~book_df['ISBN'].isin(book_read_by_user.ISBN.values)]['ISBN']
book_not_read = list(
    set(book_not_read)
    .intersection(set(books_to_books_encoded.keys()))
)

book_not_read = [[books_to_books_encoded.get(x)] for x in book_not_read]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_read), book_not_read)
)

ratings = model.predict(user_book_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_book_ids = [
    books_encoded_to_books.get(book_not_read[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Book with high ratings from user')
print('----' * 8)

top_book_user = (
    book_read_by_user.sort_values(
        by = 'book_rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)

book_df_rows = book_df[book_df['ISBN'].isin(top_book_user)]
for row in book_df_rows.itertuples():
    print(row.book_title, 'By', row.book_author)

print('----' * 8)
print('Top 10 book recommendation')
print('----' * 8)

reccomend_book = book_df[book_df['ISBN'].isin(recommended_book_ids)]
for row in reccomend_book.itertuples():
    print(row.book_title, 'By', row.book_author)

"""Model berhasil memberikan rekomendasi kepada user"""

